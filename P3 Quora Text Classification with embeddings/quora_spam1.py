# -*- coding: utf-8 -*-
"""quora spam1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F-VnxrJHGk864ZyULEXiXOf8jCcOvOFb
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import re
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

from google.colab import files
uploaded = files.upload()

import pandas as pd

data = pd.read_csv('train.csv')  # Adjust the filename if needed

# Inspect the dataset
print("Dataset Overview:")
print(data.head())
print("\nDataset Info:")
data.info()

# Preprocess the data
def clean_text(text):
    text = re.sub(r'[^a-zA-Z0-9 ]', '', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    return text

data['question_text'] = data['question_text'].apply(clean_text)

# Extract features and labels
X = data['question_text']
y = data['target']  # Assuming 'target' column contains the labels

# Train-validation split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Tokenize and pad sequences
max_words = 20000  # Vocabulary size
max_len = 100  # Maximum length of sequences

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(X_train)

X_train_seq = tokenizer.texts_to_sequences(X_train)
X_val_seq = tokenizer.texts_to_sequences(X_val)

X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)
X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)

!wget http://nlp.stanford.edu/data/glove.42B.300d.zip

!unzip glove.42B.300d.zip

glove_path = 'glove.42B.300d.txt'

embeddings_index = {}
with open(glove_path, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

print(f"Loaded {len(embeddings_index)} word vectors from GloVe.")

# Create embedding matrix
word_index = tokenizer.word_index
# Update the embedding dimension to match the GloVe file (e.g., 300)
embedding_dim = 300

# Create embedding matrix
embedding_matrix = np.zeros((max_words, embedding_dim))
for word, i in word_index.items():
    if i < max_words:
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.optimizers import Adam

# Build the model
model = Sequential()

# Embedding layer with pre-trained GloVe embeddings
model.add(Embedding(
    input_dim=max_words,           # Size of the vocabulary
    output_dim=embedding_dim,      # Dimensionality of embeddings (e.g., 300)
    weights=[embedding_matrix],    # Pre-trained GloVe matrix
    input_length=max_len,          # Max input length (padded sequence length)
    trainable=False                # Set to True if you want to fine-tune embeddings
))

# LSTM layer (Bidirectional optional)
# model.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2)))
model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))

# Fully connected layers
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))

# Output layer for binary classification
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

# Display model summary
model.summary()



# Train the model
batch_size = 32
epochs = 2

history = model.fit(
    X_train_pad, y_train,
    validation_data=(X_val_pad, y_val),
    batch_size=batch_size,
    epochs=epochs,
    verbose=1
)

# Evaluate the model
val_pred = (model.predict(X_val_pad) > 0.5).astype(int)
print("Classification Report:")
print(classification_report(y_val, val_pred))

import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend()
plt.show()



